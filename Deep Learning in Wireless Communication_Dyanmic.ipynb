{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcfc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda,Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e191b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducing result\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa0d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 4 k: 2 n: 2\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "# define (n,k) here for (n,k) autoencoder\n",
    "# n = n_channel \n",
    "# k = log2(M)  ==> so for (7,4) autoencoder n_channel = 7 and M = 2^4 = 16 \n",
    "M = 4\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "n_channel = 2\n",
    "R = k/n_channel\n",
    "print ('M:',M,'k:',k,'n:',n_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52eb9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 8000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d8d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60934734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n"
     ]
    }
   ],
   "source": [
    "# checking data shape\n",
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3c458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0. 1. 0. 0.]\n",
      "0 [1. 0. 0. 0.]\n",
      "1 [0. 1. 0. 0.]\n",
      "3 [0. 0. 0. 1.]\n",
      "1 [0. 1. 0. 0.]\n",
      "3 [0. 0. 0. 1.]\n",
      "0 [1. 0. 0. 0.]\n",
      "3 [0. 0. 0. 1.]\n",
      "0 [1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# checking generated data with it's label\n",
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a45581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91844\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# defining autoencoder and it's layer\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = Lambda(lambda x: np.sqrt(n_channel)*K.l2_normalize(x,axis=1))(encoded1)\n",
    "\n",
    "EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M,activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "adam = Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387b63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 2)                 0         \n",
      "                                                                 \n",
      " gaussian_noise (GaussianNoi  (None, 2)                0         \n",
      " se)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62\n",
      "Trainable params: 62\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# printing summary of layers and it's trainable parameters \n",
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629886c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.0332\n",
      "Epoch 2/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7331\n",
      "Epoch 3/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5552\n",
      "Epoch 4/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3227\n",
      "Epoch 5/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1519\n",
      "Epoch 6/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0826\n",
      "Epoch 7/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0535\n",
      "Epoch 8/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0387\n",
      "Epoch 9/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0329\n",
      "Epoch 10/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0241\n",
      "Epoch 11/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0206\n",
      "Epoch 12/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0163\n",
      "Epoch 13/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0167\n",
      "Epoch 14/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0128\n",
      "Epoch 15/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0114\n",
      "Epoch 16/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0110\n",
      "Epoch 17/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0117\n",
      "Epoch 18/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 19/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 20/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 21/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 22/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 23/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 24/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 25/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 26/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 27/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 28/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 29/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 30/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 31/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 32/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0049\n",
      "Epoch 33/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0066\n",
      "Epoch 34/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0063\n",
      "Epoch 35/45\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.0060\n",
      "Epoch 36/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 37/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 38/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 39/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 40/45\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 41/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0065\n",
      "Epoch 42/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0064\n",
      "Epoch 43/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0057\n",
      "Epoch 44/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0073\n",
      "Epoch 45/45\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c1bdd4790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traning auto encoder\n",
    "autoencoder.fit(data, data,\n",
    "                epochs=45,\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7351013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making decoder from full autoencoder\n",
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea2dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data for checking BER\n",
    "N = 50000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3691aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1\n"
     ]
    }
   ],
   "source": [
    "# checking generated data\n",
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89555f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving keras model\n",
    "from keras.models import load_model\n",
    "# if you want to save model then remove below comment\n",
    "# autoencoder.save('autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35852998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making encoder from full autoencoder\n",
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d75f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# for plotting learned consteallation diagram\n",
    "\n",
    "scatter_plot = []\n",
    "for i in range(0,M):\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    scatter_plot.append(encoder.predict(np.expand_dims(temp,axis=0)))\n",
    "scatter_plot = np.array(scatter_plot)\n",
    "print (scatter_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use this function for ploting constellation for higher dimenson like 7-D for (7,4) autoencoder \n",
    "x_emb = encoder.predict(test_data)\n",
    "noise_std = np.sqrt(1/(2*R*EbNo_train))\n",
    "noise = noise_std * np.random.randn(N,n_channel)\n",
    "x_emb = x_emb + noise\n",
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(learning_rate=700, n_components=2,n_iter=35000, random_state=0, perplexity=60).fit_transform(x_emb)\n",
    "print (X_embedded.shape)\n",
    "X_embedded = X_embedded / 7\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_embedded[:,0],X_embedded[:,1])\n",
    "plt.axis((-2.5,2.5,-2.5,2.5)) \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting constellation diagram\n",
    "import matplotlib.pyplot as plt\n",
    "scatter_plot = scatter_plot.reshape(M,2,1)\n",
    "plt.scatter(scatter_plot[:,0],scatter_plot[:,1])\n",
    "plt.axis((-2.5,2.5,-2.5,2.5))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b47b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating BER\n",
    "# this is optimized BER function so it can handle large number of N\n",
    "# previous code has another for loop which was making it slow\n",
    "EbNodB_range = list(frange(-4,8.5,0.5))\n",
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])\n",
    "    # use below line for generating matlab like matrix which can be copy and paste for plotting ber graph in matlab\n",
    "    print(ber[n], \" \",end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting ber curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(2,2)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70721a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving figure remove below comment\n",
    "#plt.savefig('AutoEncoder_2_2_constrained_BER_matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa56b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
